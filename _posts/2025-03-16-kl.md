---
layout: post
title: "What is KL divergence?"
date: 2025-03-16
categories: infotheory
permalink: /infotheory/kl/
author: "Joonkyu Min"
---


KL divergence, or Relative Entropy is a measure of the distance between two distributions.

It is defined as

$$
D_{KL}(p(x)\parallel q(x))=\sum_{x}p(x)\log{\frac{p(x)}{q(x)}}=\mathbb{E}_{p(x)}\left[ \log{\frac{p(x)}{q(x)}} \right]
$$

It can also be understood as difference of cross entropy and entropy.

$$
D_{KL}(p(x)\parallel q(x)) = \mathbb{E}_{p(x)}\left[ \log{\frac{1}{q(x)}} \right] - \mathbb{E}_{p(x)}\left[ \log{\frac{1}{p(x)}} \right] = \textbf{C.E.}(p, q) - H(p)
$$

It is actually not a distance metric because it doesn't hold basic properties of a metric, (1) symmetry and (2) triangle inequality.

An important property is that KL divergence is always non-negative. (Information inequality)

$$
D(p\parallel q)\ge 0, \textbf{with equality iff } p(x)=q(x)
$$

It can be proved by the Jensen inequality $f(E[X])\le E[f(X)]$.

$$
\begin{align}
D(p\parallel q) &= \sum _{x}p(x)\left(-\log{\frac{q(x)}{p(x)}}\right) \\
 & \ge -\log\left( \sum_{x}p(x){\frac{q(x)}{p(x)}}\right) (\because \text{Jensen inequality})\\
 & =-\log \sum_{x}q(x)=0
\end{align}
$$

equality holds iff $\frac{q(x)}{p(x)}$ is constant.


This leads to an important result in information theory, that **Entropy is maximized for uniform distribution**.

Simply setting $q=u=\frac{1}{|\chi|}$: uniform distribution,

$$
\begin{align}
0\le D(p\parallel q) &= \sum _{x}p(x)\left(\log{\frac{p(x)}{\frac{1}{|\chi|}}}\right) \\
 & =\log |\chi| - \sum _{x}p(x)\log{p(x)} \\
 & =\log |\chi| - H(X)
\end{align}
$$

Therefore, $H(X)\le \log|\chi|$, is maximized when $p(X)$ is also uniform distribution.
This means that most information is uniform distribution, which gives a lot of intuition in many engineering problems such as compression.

---
**Reference**

T. Cover, Elements of information theory. John Wiley & Sons, 2006.
