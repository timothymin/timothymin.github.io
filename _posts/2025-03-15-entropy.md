---
layout: post
title: "What is Entropy?"
date: 2025-03-15
categories: infotheory
permalink: /infotheory/entropy/
author: "Joonkyu Min"
---

In information theory, information measures surprise. When an event's probability is smaller, it is more surprising and informative.

Entropy is a measure of uncertainty or self information of a random variable. 
It is defined as

$$
\begin{align}
H(X) & =\sum_{x} p(x)\log \frac{1}{p(x)}=-\sum_{x} p(x)\log p(x) \\
 & = \mathbb{E}_{p}\log \frac{1}{p(x)}
\end{align}
$$

Since $0\le p(x)\le 1$, entropy satisfies $H(X)\ge 0$.

Extended to multiple random variables, joint entropy is defined 

$$
\begin{align}
H(X,Y) & =\sum_{x,y}p(x,y)\log{\frac{1}{p(x,y)}} \\
 &= -\mathbb{E}_p\log p(X,Y)
\end{align}
$$

Defined like this, if X,Y is independent,

$$
H(X,Y)=H(X)+H(Y)
$$

Also, Conditional entropy is defined

$$
\begin{align}
H(Y|X) & =\mathbb{E}_{p(x)}H(Y|X=x) \\
 & =\sum_{x}p(x)\sum_{y}p(y|x)\log{\frac{1}{p(y|x)}} \\
 & =-\mathbb{E}_{p(x,y)}\log p(Y|X)
\end{align}
$$

This definition leads to the chain rule, which is joint entropy is the sum of entropy of one random variable and conditional entropy of another.

$$
H(X,Y)=H(X)+H(Y|X)
$$

---
**Reference**

T. Cover, Elements of information theory. John Wiley & Sons, 2006.
